\section{Methods}

In our analysis, we employed three distinct regression techniques: Linear Regression,
Random Forest Regression, and XGBoost Regression. Each method offers unique
advantages and operates under different principles, allowing us to evaluate their
performance across various datasets.

Linear Regression serves as the baseline approach in regression analysis. It
models the relationship between a dependent variable and one or more independent
variables by fitting a linear equation to observed data. This simplicity facilitates
straightforward interpretation of coefficients, making it ideal for datasets
where relationships are expected to be linear. However, its performance can
decline when dealing with complex, non-linear interactions among variables.

To address non-linear relationships, we utilized Random Forest Regression, an ensemble
learning method that constructs multiple decision trees during training. By
averaging the predictions from these individual trees, Random Forests enhance predictive
accuracy and control overfitting. This method captures intricate data patterns
and interactions, providing robust performance across diverse datasets. Notably,
Random Forests are less sensitive to noisy data and typically offer stable performance
across various scenarios.
\[
	\hat{y}= \frac{1}{N_{t}}\sum_{i=1}^{N_t}h_{i}(x)
\]
Further enhancing our modeling capabilities, we applied XGBoost Regression (Extreme
Gradient Boosting), an advanced ensemble technique that builds decision trees
sequentially. Each new tree focuses on correcting errors made by its predecessors,
optimizing performance through gradient descent algorithms. XGBoost is renowned for
its speed and efficiency, often delivering superior predictive accuracy, especially
in datasets with complex structures. However, it can be more sensitive to noisy data
and may overfit if not properly regularized.

The prediction model is:
\[
	\hat{y}_{i}= \sum_{k=1}^{K}f_{k}(x_{i}), \quad f_{k}\in \mathcal{F}
\]
where $f_{k}(x_{i})$ is the prediction from the $k$-th tree, and $\mathcal{F}$ is
the space of trees defined as:
\[
	\mathcal{F}= \{f(x) = w_{q(x)}\}, \quad q(x) \to T, \quad w \in \mathbb{R}^{T}
\]
The objective function is:
\[
	\mathcal{L}= \sum_{i=1}^{n}l(y_{i}, \hat{y}_{i}) + \sum_{k=1}^{K}\Omega(f_{k}),
	\quad \Omega(f) = \gamma T + \frac{1}{2}\lambda \|w\|^{2}
\]
where $l(y_{i}, \hat{y}_{i})$ is the loss function, $\gamma$ penalizes the
number of leaves $T$, and $\lambda$ regularizes the leaf weights.

To optimize the performance of our XGBoost regression model, we implemented a
hyperparameter tuning process using grid search. This method evaluates combinations
of hyperparameters to identify the configuration that maximizes the model's predictive
accuracy.

\begin{table}[h!]
	\centering
	\caption{Tuning Parameters and Their Tested Values}
	\renewcommand{\arraystretch}{1} % Adjust row height for better readability
	\setlength{\tabcolsep}{10pt} % Adjust column spacing
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Parameter Name}    & \textbf{Tested Values}       \\
		\hline
		\texttt{n\_estimators}     & 300, 500, 700                \\
		\hline
		\texttt{max\_depth}        & 6, 8, 10                     \\
		\hline
		\texttt{learning\_rate}    & 0.1                          \\
		\hline
		\texttt{subsample}         & 1.0                          \\
		\hline
		\texttt{colsample\_bytree} & 0.4, 0.6, 0.8                \\
		\hline
		\texttt{reg\_alpha}        & 0, 0.05, 0.1, 0.15, 0.2, 0.5 \\
		\hline
		\texttt{reg\_lambda}       & 0, 0.2, 0.4, 0.6, 0.8, 1.0   \\
		\hline
	\end{tabular}
\end{table}
First, we explored tree structure by varying the number of boosting rounds (n\_estimators)
and the depth of the trees (max\_depth). These parameters helped us balance the trade-off
between underfitting (where the model is too simple to capture patterns) and overfitting
(where the model is overly complex and sensitive to noise). To further manage
overfitting, we incorporated regularization through parameters like reg\_alpha (L1
regularization) and reg\_lambda (L2 regularization), which penalize large
weights or excessive complexity.

Feature sampling was another focus, where we adjusted colsample\_bytree to test how
using fewer features per tree impacted the model's stability and robustness.
Additionally, the learning rate controlled the step size during optimization,
and subsampling was used to evaluate how reducing the number of samples affected
the modelâ€™s generalization.

To streamline the tuning process, we implemented Scikit-learn's GridSearchCV for
hyperparameter optimization. The XGBoost model was incorporated into a pipeline,
ensuring consistent preprocessing and hyperparameter testing. We applied three-fold,
five-fold, and ten-fold cross-validation to assess the model's performance
across different data splits, using the $R^{2}$ score as our evaluation metric.
This score measures the proportion of variance explained by the model, providing
a clear benchmark for predictive accuracy.

By employing these three regression methods, we aimed to capture a broad
spectrum of data relationships, from simple linear trends to complex, non-linear
interactions.