\subsection{Milestone}

At this stage in the project, I have spent weeks transforming the data and communicating with experts in the land development field to fully understand the value of this prediction model. I then spent a week developing a simple linear regression model to predict the sale price of parcels of land. This is NOT the final target value, however, I wanted to test prediction models on the data to initially grasp how the model will work, what the data processing looks like, and additional concerns that might arise during the training.

This lead me to statistically analyze the features within the dataset and start the data augmentation process which was described above. Although they are not included in this milestone report, a distribution plot was created for each feature and was inspected for quality purposes in regards to model variance. Many features were removed when creating the augmented dataset as many of the cells were empty, or contained the same value for every record. These features would provided little information towards training the weights.

The Distance Matrix API within Google Cloud's API \& Services is currently being used to engineer an additional feature that stores the distance between the owner's residence, and the parcel of the land they own. This will be my final target value, and I have cross-referenced this feature design with field experts to back the decision.

With the dataset obtained and new features developed, I modified the existing simple linear regression model to set a new starting point in this development phase. The results were not promising as I received high MSE values in both the training and test sets, indicating significant prediction errors. Additionally, low extremely $R^2$ scores were reported on training data suggesting that the model is barely able to explain the variance in the training set. This could mean the features are not informative enough to predict the target variable or the relationship between features and the target variable is non-existent or very weak. There is evidence within this industry to prove the relationship between the distance between an owner's residence and the parcel's address and the length of time they possess the parcel of land under their name, however, this model is not able to capture this relationship.

With these results and initial assessment, I started tracking the individual predictions of the model. The first issue I recognized was that some predictions from the model produced negative distances, which is not possible. This could be the result of many reasons, and my analysis is described below:
\begin{enumerate}
    \item \textbf{Too much variance with the features included in the dataset and the target value.} For testing purposes, I decreased the number of features included in the dataset to try and combat this issue. I noticed a lower MSE error for both sets, however, they were marginally small changes.
    \item \textbf{To ensure that the linear regression model does not predict negative distances (since distances cannot be negative), I transformed the target feature using log algorithms.} I applied the log function with an small epsilon value to the feature before training, and reversed the process after predictions. This modification, however, did not work well as many distance values were zero, meaning the transformed distance value was equal to the epsilon value, resulting in almost all predictions equaling epsilon. This left me with the same prediction error as before, except the model is predicting epsilon and not zero.
    \item \textbf{L1 or L2 regularization in models can stabilize predictions and reduce noise.} With some extreme cases of negative predictions, I applied the ridge regression with L2 regularization during training and noticed an increase the $R^2$ score and more values along the $y=x$ line when plotting predicted vs actual target values. However, I experienced a negative impact towards the values that mattered more than the majority of the examples with a distance equal to zero. This lead me to believe the target feature distribution of my examples were not spread equally.
    \item \textbf{Distance distribution values are not uniformly scattered between the min and max distance value.} Due to the fact that many owners live at the same address as the land parcels they own, a majority of the records have a target value of zero, while a minority of them have a value greater than zero. The goal of the model is to predict the distance between the owner's resident and the parcel's address leading me to conclude that many of my examples have a target value of zero, and the model is having a hard time finding the relationship between features when the distance is greater than zero. I dove deep into this analysis by first understanding the distribution of distances in the dataset through plots in a similar fashion of what I applied to the original dataset features before augmentation. Over 90\% of the data was examples with a distance of 0, raising a major concern that needs to be addressed. 
    \item \textit{(FUTURE)} \textbf{Stratified Sampling can be used to divide the augmented dataset into distance categories of [0, 1-50, 50-99, 100-149, 150-200].} The intention is to balance the training and testing set distributions of the target feature value between the min and max.
    \item \textit{(FUTURE)} \textbf{Weighted Loss Function can be used to modify the loss function to give higher weight to examples from underrepresented distances.} This approach can be used if the original dataset size is not big enough to create a uniformly distributed dataset across target values.
    \end{enumerate}
    
Additionally, I trained a simple random forest regression model and ran it after the linear regression model for each change/modification listed above to see if the issue was related to the shape of the model being linear or not. I have not come to any conclusion yet based on the generated outputs. 

My biggest concern right now is addressing the value distribution across target values during training and testing. The relationship between the target value and features exists within the industry and is a usable quantitative value during investment decisions, however, minimal models have been created to combat this gap. I am balancing both the model development aspect of the project as well as the data quality used during training. Many features are accessible to me and it is a matter of filtering the data to ensure the relationships benefit the prediction model and are not a hindrance to its variation.

Currently, the model's accuracy is not usable and the relationship between the features and target value has not been found. There is a lot of room for testing in regards to the additional methods and algorithms during this developmental process, however, I am focusing on the dataset architecture and the high-level scope of the model with the goal of getting a "good" prediction. From there, I can use complementary methods or algorithms to better the accuracy and usability of the model in the industry. 
