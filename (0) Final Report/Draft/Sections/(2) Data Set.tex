\section{Data set }

\subsection{Data Collection and Sources}
\textbf{Primary Data Source: } The dataset utilized in this project was sourced through a subscription to Latapult, a Geographic Information System (GIS) platform specializing in land development and real estate data. Latapult provides detailed information on property records, zoning regulations, land use, and market trends. According to Latapult's End User License Agreement (EULA), users are granted a limited, non-transferable license to use the Latapult database.

\textbf{Additional Data Source: } The Google Cloud Distance Matrix API is used to calculate distances between landowners’ residences and their parcels of land. All requests were authenticated via API key and complied with Google Cloud's Terms of Service.

\textbf{Scope of the Data: } The dataset covers the geographical region of the greater Charlotte area in North Carolina. The data sourced from Latapult is specific to this region, providing detailed insights into property records, ownership history, and transactional timelines. Some aspects of the dataset include property purchase and sale dates, allowing for an analysis of land ownership duration over time.

\subsection{Data Preprocessing}

My \textbf{transform\_dataset} function takes raw real estate data, transforms it into a structured format, and outputs a clean dataset ready for model training and analysis. The transformation process begins by removing unnecessary features, such as Assessed Improvement Value Ratio, Mortgage Date, and Condition, which are deemed irrelevant or redundant for the predictive task.

A significant aspect of the transformation involves feature engineering. For instance, the function calculates the “Ownership Distance” by determining the number of miles between an owner's residence and the land asset. Additionally, financial ratios such as the “Loan-to-Value Ratio” and “Tax-to-Value Ratio” are computed to capture the financial stress or investment characteristics of the property, which can influence selling behavior. 

\textit{(FUTURE)} To prepare categorical variables for regression modeling, one option is to apply one-hot encoding to nominal features like Zoning Type and Land Use. This encoding process converts categorical data into numerical form while preserving the original information, ensuring compatibility with machine learning algorithms. 

Additionally, numerical features are standardized using a StandardScaler to normalize their distributions. Normalization is essential for regression models as it prevents features with larger scales from dominating the learning process.

\subsection{Statistical Summaries}

My \textbf{analyze\_transformed\_dataset} function and its helper methods provide a framework for exploring and summarizing the features by calculating statistics and generating visualizations of feature distributions. By focusing on numerical features, the function computes measures of central tendency (mean, median, mode) and dispersion (standard deviation, variance, range, and interquartile range). The statistical summaries are included at the end of this report.

\subsection{Data Splitting}
\textbf{Training and Test Sets: } For now, the training to testing ratio is 80 to 20 respectively. This ratio is set based on the standards explained in class, however, the opportunity to modify the ratio will be a future decision when I am confident enough in analyzing the bias and variance reports of the model's outputs. As I increase the amount of data being trained, the ratio will increase to 90 or 99 to 10 or 1 respectively. The current size of the dataset is 100 examples randomly pulled from a downloaded .csv file of over 5000 examples.
